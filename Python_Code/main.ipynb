{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master's thesis Psychology  \n",
    "University of Zurich  \n",
    "Author: Berit Barthelmes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "\n",
    "from fetch_articles_crossref import fetch_articles_crossref\n",
    "from fetch_articles_ebsco import fetch_articles_ebsco \n",
    "from fetch_articles_scihub import fetch_articles_scihub\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DOI data from file and fetch from crossref and scihub\n",
    "\n",
    "with open('../crossref_relevant_dois.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    dois = list(reader)[0][1:]\n",
    "    print(dois)\n",
    "    fetch_articles_crossref(dois)\n",
    "    fetch_articles_scihub(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch available articles from ebsco\n",
    "\n",
    "fetch_articles_ebsco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to retrieve the DOIs of all unavailable articles based on an XML document \n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def ebsco_get_unavailable_article_dois(filepath):\n",
    "  tree = ET.parse(filepath)\n",
    "  \n",
    "  # get root element\n",
    "  root = tree.getroot()\n",
    "\n",
    "  # iterate over articles\n",
    "  unavailable_article_dois = []\n",
    "\n",
    "  for article in root.findall(\"rec\"):\n",
    "    # iterate child elements of item\n",
    "    formats = article.find(\".//header/controlInfo/artinfo/formats\")\n",
    "    if formats is None:\n",
    "      doi = article.find(\".//ui[@type='doi']\")\n",
    "      if doi is not None:\n",
    "        unavailable_article_dois.append(doi.text)\n",
    "\n",
    "  return unavailable_article_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which articles are not available to download from ebsco according to the XML overview file\n",
    "\n",
    "ebsco_search_results = \"../ebsco_articles.xml\"\n",
    "ebsco_unavailable_article_dois = ebsco_get_unavailable_article_dois(ebsco_search_results)\n",
    "print(\"Articles that are not retrievable: \", len(ebsco_unavailable_article_dois))\n",
    "print(ebsco_unavailable_article_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch unavailable ebsco articles from scihub\n",
    "\n",
    "fetch_articles_scihub(ebsco_unavailable_article_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of articles for doc2vec (deprecated)\n",
    "\n",
    "# read in XML article files and extract plain text to use it for doc2vec\n",
    "# plain text is then saved as a .txt file with paragraphs seperated by a double line break\n",
    "\n",
    "# xml_dirs = [os.fsencode('../articles_xml/crossref/api/'),\n",
    "#             os.fsencode('../articles_xml/crossref/scihub/'),\n",
    "#             os.fsencode('../articles_xml/ebsco/website/'),\n",
    "#             os.fsencode('../articles_xml/ebsco/scihub/')]\n",
    "\n",
    "# for xml_dir in xml_dirs:\n",
    "#     for file in os.listdir(xml_dir):\n",
    "#         directory = os.fsdecode(xml_dir)\n",
    "#         print(file)\n",
    "#         filename = os.fsdecode(file)\n",
    "#         path = os.path.join(directory, filename)\n",
    "#         if filename.endswith(\".xml\"):\n",
    "#             paragraphs = helpers.xml_to_txt(path)\n",
    "#             txt_file_path = f\"{os.path.splitext(path)[0]}.txt\"\n",
    "#             txt_file_path = re.sub(\"\\.\\.\\/articles_xml\",\n",
    "#                                    \"../articles_txt\", txt_file_path)\n",
    "#             print(txt_file_path)\n",
    "#             with open(txt_file_path, \"w\") as f:\n",
    "#                 for i, paragraph in enumerate(paragraphs):\n",
    "#                     if paragraph is not None:\n",
    "#                         # print(paragraph)\n",
    "#                         f.write(f\"{paragraph}\")\n",
    "#                         if i != len(paragraphs)-1:\n",
    "#                             f.write(f\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity calculation with the help of doc2vec (deprecated)\n",
    "\n",
    "# from gensim.models import doc2vec\n",
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "# import numpy as np\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# import os\n",
    "\n",
    "\n",
    "# def generate_embeddings(corpus):\n",
    "#     tagged_documents = []\n",
    "\n",
    "#     for paragraph_id, paragraph in enumerate(corpus):\n",
    "#         words = paragraph.split()\n",
    "#         tagged_documents.append(TaggedDocument(words, tags=[\"{0:0>4}\".format(paragraph_id)]))\n",
    "#     print(\"Paragraph count: \", len(tagged_documents))\n",
    "\n",
    "#     d2v_model = doc2vec.Doc2Vec(vector_size=50, min_count=10, epochs=30)\n",
    "#     d2v_model.build_vocab(tagged_documents)\n",
    "\n",
    "#     d2v_model.train(tagged_documents, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "\n",
    "#     print(\"Vector count: \", len(d2v_model.dv))\n",
    "#     return d2v_model.dv\n",
    "\n",
    "# def calc_cos_similarities(embeddings):\n",
    "#     q_emb = embeddings[0]\n",
    "#     cos_similarities = []\n",
    "#     i = 0\n",
    "#     for i in range(1, len(embeddings)):\n",
    "#         p_emb = embeddings[i]\n",
    "#         cos_similarities.append(np.dot(q_emb, p_emb)/(norm(q_emb)*norm(p_emb)))\n",
    "#     return cos_similarities\n",
    "\n",
    "\n",
    "# query = \"\"\n",
    "\n",
    "# with open(\"../memory_decay_query.txt\", \"r\") as f:\n",
    "#     query = f.read()\n",
    "\n",
    "# txt_dirs = [os.fsencode('../articles_txt/crossref/api/'),\n",
    "#             os.fsencode('../articles_txt/crossref/scihub/'),\n",
    "#             os.fsencode('../articles_txt/ebsco/website/'),\n",
    "#             os.fsencode('../articles_txt/ebsco/scihub/')]\n",
    "\n",
    "# for txt_dir in txt_dirs:\n",
    "#     for file in os.listdir(txt_dir): #[0:10]:\n",
    "#         directory = os.fsdecode(txt_dir)\n",
    "#         # print(file)\n",
    "#         filename = os.fsdecode(file)\n",
    "#         path = os.path.join(directory, filename)\n",
    "#         if filename.endswith(\".txt\"):\n",
    "#             embeddings = \"\"\n",
    "            \n",
    "#             with open(path, \"r\") as f:\n",
    "#                 paragraphs = f.read().split(\"\\n\\n\")\n",
    "#                 # include the query in the text corpus\n",
    "#                 corpus = paragraphs[:]\n",
    "#                 corpus.insert(0, query)\n",
    "#                 embeddings = generate_embeddings(corpus)\n",
    "#                 print(len(embeddings))\n",
    "            \n",
    "#             cos_similarities = calc_cos_similarities(embeddings)\n",
    "#             print(len(cos_similarities))\n",
    "#             relevant_count = np.minimum(5, len(cos_similarities))\n",
    "#             relevant_indices = np.argpartition(cos_similarities, -relevant_count)[-relevant_count:]\n",
    "#             print(relevant_indices)\n",
    "#             relevant_paragraphs = [paragraphs[i] for i in relevant_indices]\n",
    "\n",
    "#             # Write most relevant paragraphs to .txt file\n",
    "#             txt_file_path = re.sub(\"\\.\\.\\/articles_txt\",\n",
    "#                                    \"../articles_paragraphs\", path)\n",
    "#             # print(txt_file_path)\n",
    "#             with open(txt_file_path, \"w\") as f:\n",
    "#                 for i, paragraph in enumerate(relevant_paragraphs):\n",
    "#                     # if paragraph is not None:\n",
    "#                         # print(paragraph)\n",
    "#                     f.write(f\"{paragraph}\")\n",
    "#                     if i != len(paragraphs)-1:\n",
    "#                         f.write(f\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GPT4 functionality for embedding and rating of paragraphs\n",
    "\n",
    "import os \n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import openai\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"GPT4_KEY\")\n",
    "\n",
    "# Number of articles to process (embedding/rating)\n",
    "M = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries the vector embeddings to find the 5 most relevant paragraphs for the classification task\n",
    "\n",
    "def search_paragraphs(df, query, n=5):\n",
    "    query_embedding = get_embedding(\n",
    "        query,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df[\"similarity\"] = df['embeddings'].apply(lambda x: cosine_similarity(x, query_embedding))\n",
    "\n",
    "    results = (\n",
    "        df.sort_values(\"similarity\", ascending=False)\n",
    "        .head(n)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    return df, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file and extract the text\n",
    "\n",
    "xml_dir = \"../articles_xml_final_211023/\"\n",
    "input_csv = pd.read_csv(\"../pre_result.csv\")\n",
    "dfs = []\n",
    "i = 0\n",
    "\n",
    "for i, row in input_csv.iterrows():\n",
    "  filename = row[\"filename\"]\n",
    "  tree = ET.parse(f\"{xml_dir}{filename}\")\n",
    "  root = tree.getroot()\n",
    "\n",
    "  # finds all p tags in namespace http://www.tei-c.org/ns/1.0 and creates a list of these tags.\n",
    "  paragraphs = root.findall(\".//{http://www.tei-c.org/ns/1.0}p\")\n",
    "\n",
    "  # extracts the text content of each paragraph tag and stores it in a new list called paragraphs.\n",
    "  paragraphs = [''.join(p.itertext()) for p in paragraphs]\n",
    "\n",
    "  # create a pandas data.frame with one column for the paragraphs and appends it to a list of data frames (one for each article)\n",
    "  # only create data.frame if the article is not empty\n",
    "  if paragraphs:\n",
    "      i+=1\n",
    "      df = pd.DataFrame(paragraphs, columns=['paragraphs'])\n",
    "      df.insert(0, 'filename', filename)\n",
    "      dfs.append(df)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the paragraphs into embeddings & save embeddings as csv\n",
    "\n",
    "for df in dfs:\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, model=\"text-embedding-ada-002\", max_retries=10, request_timeout=20000)\n",
    "    df['embeddings'] = embeddings.embed_documents(df[\"paragraphs\"].tolist())\n",
    "    print('Article name: ', df['filename'][0])\n",
    "    print('# of paragraphs: ', len(df['embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv with embeddings\n",
    "\n",
    "query = \"\"\"The concept of memory decay in scientific psychology describes that \\\n",
    "        memory traces are stored with an initial strength value and that this strength decays passively over time unless it is reactivated. \\\n",
    "        Reactivation of memory traces according to the memory decay theory can be done by practice. Once the activation level for a stored memory trace becomes too low, the memory trace is lost. \\\n",
    "        The memory decay theory concerns memory loss in healthy individuals. Changes solely due to aging processes and abnormal changes in memory capacity due to impairments like dementia are not the explanatory focus of this theory.\"\"\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df, res = search_paragraphs(df, query, n=3)\n",
    "    res = res.reset_index()\n",
    "    dfs[i] = res\n",
    "    print(res['paragraphs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append paragraph, paragraph embeddings, cosine similarities, GPT4 rating, GPT4 rationale columns to csv\n",
    "\n",
    "final_dfs = []\n",
    "\n",
    "for df in dfs:\n",
    "  input_df = input_csv.loc[input_csv['filename'] == df['filename'][0]].copy()\n",
    "\n",
    "  row1_df = df.iloc[0].to_frame().T\n",
    "  row2_df = df.iloc[1].to_frame().T\n",
    "  row3_df = df.iloc[2].to_frame().T\n",
    "\n",
    "  row1_df.reset_index(drop=True, inplace=True)\n",
    "  row2_df.reset_index(drop=True, inplace=True)\n",
    "  row3_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  new_df = pd.DataFrame({\"p1\": row1_df[\"paragraphs\"], \"p1_embedding\": row1_df[\"embeddings\"], \"p1_cos_similarity\": row1_df[\"similarity\"], \"p1_rating_category\": \"\", \"p1_rating_rationale\": \"\", \n",
    "                        \"p2\": row2_df[\"paragraphs\"], \"p2_embedding\": row2_df[\"embeddings\"], \"p2_cos_similarity\": row2_df[\"similarity\"], \"p2_rating_category\": \"\", \"p2_rating_rationale\": \"\", \n",
    "                        \"p3\": row3_df[\"paragraphs\"], \"p3_embedding\": row3_df[\"embeddings\"], \"p3_cos_similarity\": row3_df[\"similarity\"], \"p3_rating_category\": \"\", \"p3_rating_rationale\": \"\"},\n",
    "                        index=[0])\n",
    "  \n",
    "  final_dfs.append(new_df)\n",
    "\n",
    "df_embeddings_similarities = pd.concat(final_dfs, ignore_index=True)\n",
    "input_csv_with_embeddings_similiarities = pd.merge(input_csv, df_embeddings_similarities, how=\"outer\", left_index=True, right_index=True)\n",
    "\n",
    "input_csv_with_embeddings_similiarities.to_csv(\"final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "454f3656adfa09446b5a0fb8c6cd14908060e63f590ff1abf918cac063f9fd01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
